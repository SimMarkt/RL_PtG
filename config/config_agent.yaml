# ----------------------------------------------------------------------------------------------------------------
# RL_PtG: Deep Reinforcement Learning for Power-to-Gas dispatch optimization
# https://github.com/SimMarkt/RL_PtG

# config_agent: 
# > Configuration file for the RL agent in the PtG-CH4 environment
# > RL_PtG works with the Stable-Baselines3 and SB3 Contrib libraries, for more information, refer to:
#       Stable-Baselines3 : https://stable-baselines3.readthedocs.io/en/master/guide/algos.html
#       SB3 Contrib: https://sb3-contrib.readthedocs.io/en/master/
# ----------------------------------------------------------------------------------------------------------------

n_envs : 6                   # Number environment instances/workers for training
rl_alg : SAC                 # selected RL algorithm - already implemented [DQN, A2C, PPO, TD3, SAC, TQC]

hyperparameters:    # Implemented hyperparameters of the RL algorithms used
    DQN :
        action_type : discrete          # the type of action: "discrete" or "continuous"
        alpha : 0.0002                  # Learning rate (_al)
        gamma : 0.9728                  # Discount factor (_ga)
        buffer_size : 1000000           # Replay buffer size (_rbf)
        batch_size : 544                # batch size
        eps_init : 0.1817               # initial exploration coefficient
        eps_fin : 0.00268               # final exploration coefficient
        eps_fra : 0.00434               # ratio of the training set for exploration annealing
        hidden_layers : 7               # No. of hidden layers
        hidden_units : 366              # No. of hidden units
        activation : ReLU               # activation function ('ReLU' or 'Tanh')
        train_freq : 5                  # training frequency
        target_update_interval : 113204 # target network update interval
        tau : 1.0                       # soft update parameter
        learning_starts : 50000         # No. of steps before starting the training of actor/critic networks
    A2C :
        action_type : discrete        # the type of action: "discrete" or "continuous"
        alpha : 0.00021                 # Learning rate (_al)
        gamma : 0.9393                  # Discount factor (_ga)
        ent_coeff : 0.00004             # entropy coefficient
        n_steps : 658                   # No. of steps of the n-step TD update
        hidden_layers : 4               # No. of hidden layers
        hidden_units : 808              # No. of hidden units
        activation : Tanh               # activation function ('ReLU' or 'Tanh')
        gae_lambda : 0.9819             # factor for generalized advantage estimation
        normalize_advantage : True      # normalize advantage ('False' or 'True')
        gSDE : False                    # gSDE exploration ('False' or 'True')
    PPO : 
        action_type : discrete        # the type of action: "discrete" or "continuous"
        alpha : 0.00005                 # Learning rate (_al)
        gamma : 0.973                   # Discount factor (_ga)
        ent_coeff : 0.00001             # entropy coefficient 
        n_steps_f : 21                  # factor for determining the No. of steps of the n-step TD update (n_steps = n_steps_f * batch_size)
        batch_size : 203                # batch size
        hidden_layers : 2               # No. of hidden layers
        hidden_units : 358              # No. of hidden units
        activation : ReLU               # activation function ('ReLU' or 'Tanh')
        gae_lambda : 0.8002             # factor for generalized advantage estimation
        n_epoch : 13                    # No. of epochs for mini batch training
        normalize_advantage : False     # normalize advantage ('False' or 'True')
        gSDE : False                    # gSDE exploration ('False' or 'True')
    TD3 :
        action_type : continuous      # the type of action: "discrete" or "continuous"
        alpha : 0.00015                 # Learning rate (_al)
        gamma : 0.9595                  # Discount factor (_ga)
        buffer_size : 19984440          # Replay buffer size (_rbf)
        batch_size : 257                # batch size
        hidden_layers : 3               # No. of hidden layers
        hidden_units : 743              # No. of hidden units
        activation : ReLU               # activation function ('ReLU' or 'Tanh')
        train_freq : 9                  # training frequency
        tau : 0.005                     # soft update parameter
        learning_starts : 100           # No. of steps before starting the training of actor/critic networks
        sigma_exp : 0.19                # exploration noise in the target          
    SAC :
        action_type : continuous      # the type of action: "discrete" or "continuous"
        alpha : 0.0001                  # Learning rate (_al)
        gamma : 0.9628                  # Discount factor (_ga)
        ent_coeff : auto                # entropy coefficient (choose either values within [0,1] or the automatic adjustment using 'auto')
        buffer_size : 2267734           # Replay buffer size (_rbf)
        batch_size : 470                # batch size
        hidden_layers : 6               # No. of hidden layers
        hidden_units : 233              # No. of hidden units
        activation : Tanh               # activation function ('ReLU' or 'Tanh')
        train_freq : 1                  # training frequency
        tau : 0.005                     # soft update parameter
        learning_starts : 100           # No. of steps before starting the training of actor/critic networks
        gSDE : False                    # gSDE exploration ('False' or 'True')
    TQC :
        action_type : continuous      # the type of action: "discrete" or "continuous"
        alpha : 0.00044                 # Learning rate (_al)
        gamma : 0.9639                  # Discount factor (_ga)
        ent_coeff : 0.00047             # entropy coefficient  (choose either values within [0,1] or the automatic adjustment using [2])
        buffer_size : 6165170           # Replay buffer size (_rbf)
        batch_size : 290                # batch size
        hidden_layers : 3               # No. of hidden layers
        hidden_units : 708              # No. of hidden units
        activation : ReLU               # activation function ('ReLU' or 'Tanh')
        top_quantiles_drop : 2          # No. of top quantiles to drop
        n_quantiles : 30                # No. of quantiles for distributed value estimation
        train_freq : 1                  # training frequency
        n_critics : 2                   # No. of critics
        tau : 0.005                     # soft update parameter
        learning_starts : 100           # No. of steps before starting the training of actor/critic networks
        gSDE : False                    # gSDE exploration ('False' or 'True')
