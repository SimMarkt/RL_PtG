# ----------------------------------------------------------------------------------------------------------------
# RL_PtG: Deep Reinforcement Learning for Power-to-Gas dispatch optimization
# https://github.com/SimMarkt/RL_PtG

# config_agent: 
# > Configuration file for the RL agent in the PtG-CH4 environment
# 
# For adding additional algorithms, take the following steps:
# 1. Extent the "hyperparameters" dictionary with the algorithm as a new key entry incl. hyperparameters, aligned with the structure below and
#       Stable-Baselines3 : https://stable-baselines3.readthedocs.io/en/master/guide/algos.html
#       SB3 Contrib: https://sb3-contrib.readthedocs.io/en/master/
# 2. Include the new algorithm in src/rl_agent_param.py -> def load_model(self)
# 3. Add... param_short??
# ----------------------------------------------------------------------------------------------------------------

n_envs : 6                   # Number environments/workers for training
rl_alg : TQC                 # selected RL algorithm - already implemented [DQN, A2C, PPO, TD3, SAC, TQC]
sim_step : 600               # Frequency for taking an action in [s]

hyperparameters:    # Implemented hyperparameters of the RL algorithms used
    DQN :
        action_type : discrete        # the type of action: "discrete" or "continuous"
        alpha : 0.0002                  # learning rate
        gamma : 0.9728                  # discount factor
        buffer_size : 1000000           # replay buffer size
        batch_size : 544                # batch size
        eps_init : 0.1817               # initial exploration coefficient
        eps_fin : 0.00268               # final exploration coefficient
        eps_fra : 0.00434               # ratio of the training set for exploration annealing
        hidden_layers : 7               # No. of hidden layers
        hidden_units : 366              # No. of hidden units
        activation : 0                  # activation function (0: ReLU, 1: Tanh)
        train_freq : 5                  # training frequency
        target_update_interval : 113204 # target network update interval
        tau : 1.0                       # soft update parameter
        learning_starts : 50000         # No. of steps before starting the training of actor/critic networks
    A2C :
        action_type : discrete        # the type of action: "discrete" or "continuous"
        alpha : 0.00021                 # learning rate
        gamma : 0.9393                  # discount factor
        ent_coeff : 0.00004             # entropy coefficient
        n_steps : 658                   # No. of steps of the n-step TD update
        hidden_layers : 4               # No. of hidden layers
        hidden_units : 808              # No. of hidden units
        activation : 1                  # activation function (0: ReLU, 1: Tanh)
        gae_lambda : 0.9819             # factor for generalized advantage estimation
        normalize_advantage : 1         # normalize advantage (0: Off, 1: On)
        gSDE : 0                        # gSDE exploration  (0: False, 1: True)
    PPO : 
        action_type : discrete        # the type of action: "discrete" or "continuous"
        alpha : 0.00005                 # learning rate
        gamma : 0.973                   # discount factor
        ent_coeff : 0.00001             # entropy coefficient 
        n_steps_f : 21                  # factor for determining the No. of steps of the n-step TD update (n_steps = n_steps_f * batch_size)
        batch_size : 203                # batch size
        hidden_layers : 2               # No. of hidden layers
        hidden_units : 358              # No. of hidden units
        activation : 0                  # activation function (0: ReLU, 1: Tanh)
        gae_lambda : 0.8002             # factor for generalized advantage estimation
        n_epoch : 13                    # No. of epochs for mini batch training
        normalize_advantage : 0         # normalize advantage (0: Off, 1: On)
        gSDE : 0                        # gSDE exploration  (0: False, 1: True)
    TD3 :
        action_type : continuous      # the type of action: "discrete" or "continuous"
        alpha : 0.00015                 # learning rate
        gamma : 0.9595                  # discount factor
        buffer_size : 19984440          # replay buffer size
        batch_size : 257                # batch size
        hidden_layers : 3               # No. of hidden layers
        hidden_units : 743              # No. of hidden units
        activation : 0                  # activation function (0: ReLU, 1: Tanh)
        train_freq : 9                  # training frequency
        tau : 0.005                     # soft update parameter
        learning_starts : 100           # No. of steps before starting the training of actor/critic networks
        sigma_exp : 0.19                # exploration noise in the target          
    SAC :
        action_type : continuous      # the type of action: "discrete" or "continuous"
        alpha : 0.0001                  # learning rate
        gamma : 0.9628                  # discount factor
        ent_coeff : auto                # entropy coefficient (choose either values within [0,1] or the automatic adjustment using 'auto')
        buffer_size : 2267734           # replay buffer size
        batch_size : 470                # batch size
        hidden_layers : 6               # No. of hidden layers
        hidden_units : 233              # No. of hidden units
        activation : 1                  # activation function (0: ReLU, 1: Tanh)
        train_freq : 1                  # training frequency
        tau : 0.005                     # soft update parameter
        learning_starts : 100           # No. of steps before starting the training of actor/critic networks
        gSDE : 0                        # gSDE exploration  (0: False, 1: True)
    TQC :
        action_type : continuous      # the type of action: "discrete" or "continuous"
        alpha : 0.00044                 # learning rate
        gamma : 0.9639                  # discount factor
        ent_coeff : 0.00047             # entropy coefficient  (choose either values within [0,1] or the automatic adjustment using [2])
        buffer_size : 6165170           # replay buffer size
        batch_size : 290                # batch size
        hidden_layers : 3               # No. of hidden layers
        hidden_units : 708              # No. of hidden units
        activation : 0                  # activation function (0: ReLU, 1: Tanh)
        top_quantiles_drop : 2          # No. of top quantiles to drop
        n_quantiles : 30                # No. of quantiles for distributed value estimation
        train_freq : 1                  # training frequency
        n_critics : 2                   # No. of critics
        tau : 0.005                     # soft update parameter
        learning_starts : 100           # No. of steps before starting the training of actor/critic networks
        gSDE : 0                        # gSDE exploration  (0: False, 1: True)
